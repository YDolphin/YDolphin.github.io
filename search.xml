<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Python 中有关于变量泄露的问题]]></title>
    <url>%2F2019%2F01%2F12%2FPython%2FPython%20%E4%B8%AD%E6%9C%89%E5%85%B3%E4%BA%8E%E5%8F%98%E9%87%8F%E6%B3%84%E9%9C%B2%E7%9A%84%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[Python 中有关于变量泄露的问题环境：Python 2.x语法：列表推导中 | 生成器表达式 | 集合推导 | 字典推导操作：for 关键词之后的赋值操作影响：上下文中的同名变量 123456789Python 2.7.10 (default, Aug 17 2018, 19:45:58)[GCC 4.2.1 Compatible Apple LLVM 10.0.0 (clang-1000.0.42)] on darwinType "help", "copyright", "credits" or "license" for more information.&gt;&gt;&gt; x = 'abc'&gt;&gt;&gt; y = [x for x in '123']&gt;&gt;&gt; x'3'&gt;&gt;&gt; y['1', '2', '3'] 同样的情况，在 Python 3.x 的环境下，不会发生。WHY：Python3 中这些语法都有自己的局部作用域，就像函数似的。 123456789Python 3.7.0 (default, Sep 18 2018, 18:47:22)[Clang 9.1.0 (clang-902.0.39.2)] on darwinType "help", "copyright", "credits" or "license" for more information.&gt;&gt;&gt; x = 'abc'&gt;&gt;&gt; y = [x for x in '123']&gt;&gt;&gt; x # x 的值被保留了'abc'&gt;&gt;&gt; y # 列表推导也创建了正确的列表['1', '2', '3'] PS：列表推导：构建列表的快捷方式；生成器表达式：创建其他任何类型的序列。 PSS：列表推导 &amp; 生成器表达式 ”更具可读性”《fluent python》中推荐使用，使用的原则：只用列表推导来创建新的列表，并且尽量保持简短，最好不超过两行。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>变量泄露</tag>
        <tag>列表推导中</tag>
        <tag>生成器表达式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 列表推导 & 生成器表达式]]></title>
    <url>%2F2019%2F01%2F12%2FPython%2FPython%20%E5%88%97%E8%A1%A8%E6%8E%A8%E5%AF%BC%20%26%20%E7%94%9F%E6%88%90%E5%99%A8%E8%A1%A8%E8%BE%BE%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[Python 列表推导 &amp; 生成器表达式推荐使用：因为列表推导 &amp; 生成器表达式 ”更具可读性”《Fluent Python》中推荐使用， 1234567str = 'abc'codes = []for s in str:truecodes.append(s)# 列表推导：是不是更具可读性？codes = [s for s in str] 使用的原则：只用列表推导来创建新的列表，并且尽量保持简短，最好不超过两行。 列表推导：构建列表的快捷方式；生成器表达式：创建其他任何类型的序列。 语法区别：前者是方括号 “[ ]”，后者是圆括号 ”( )” 12345678colors = ['blank', 'white']sizes = ['S', 'M', 'L']# 列表推导tshirt = [(color, size) for color in colors for size in sizes]type(tshirt) # list 对象# 生成器表达式tshirt = ('%s %s' % (c, s) for c in colors for s in sizes)type(tshirt) # 生成器(Generators)]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>生成器表达式</tag>
        <tag>列表推导</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python3网络爬虫工具安装（Mac）]]></title>
    <url>%2F2019%2F01%2F12%2FPython%2FPython%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E6%8A%80%E6%9C%AF%2FPython3%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E5%B7%A5%E5%85%B7%E5%AE%89%E8%A3%85%EF%BC%88Mac%EF%BC%89%2F</url>
    <content type="text"><![CDATA[Python3网络爬虫工具安装（Mac）以下都是基于 Python3 爬虫：抓取页面 -&gt; 分析页面 -&gt; 存储数据 请求库的安装 Homebrew 安装Mac下的包管理工具 1/usr/bin/ruby -e "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)" Python3 安装 12# Python3 &amp; pip3 一起安装brew install python3 requests 安装第三方库，Python不会自带这个库 1pip3 install requests Selenium 安装自动化测试工具，驱动浏览器执行特定的动作 1pip3 install selenium ChromeDriver 安装自动化测试工具，配置浏览器使用 12345678# 查看 chrome 版本以及对应的 chromedriver 版本http://chromedriver.chromium.org/downloads# 配置环境变量sudo mv chromedriver /usr/local/binvim ~/.bash_profileexport PATH=/usr/local/bin:$PATH# 使环境变量生效source ~/.bash_profile aiohttp 安装requests 是一个 ::阻塞式 HTTP 请求库:: ，发出一个请求后，程序会一直等待服务器响应，直到得到响应后，才会进行下一步处理。一个::异步 Web 服务::的库，异步操作可以借助 ::async/await:: 关键字，使写法更简洁。使用场景：维护一个代理池时，利用异步方式检测大量代理的运行状况，会极大地提升效率。 123pip3 install aiohttp# 官方还推荐安装以下两个库：cchardet（字符编码检测库）、aiodns（加速 DNS 的解析库）pip3 install cchardet aiodns 解析库的安装 lxml 安装Beautiful Soup、Scrapy 框架都需要用到此库。 123pip3 install lxml# 如果产生错误，需要先执行以下命令xcode-select --install Beautiful Soup 安装 123pip3 install beautifulsoup4# 注意导入的时候from bs4 import BeautifulSoup pyquery 安装 1pip3 install pyquery tesserocr 安装爬虫遇到::验证码::时，可以直接使用 OCR 来识别。（OCR：光学字符识别） 123brew install imagemagickbrew install tesseract --all-languagespip3 install tesserocr pillow 数据库的安装 MySQL 安装轻量级关系型数据库 1234brew install mysqlsudo mysql.server startsudo mysql.server stopsudo mysql.server restart MongoDB 安装::C++:: 编写的非关系型数据库，基于::分布式文件存储::。 12345brew install mongodbbrew services start mongodbsudo mongodbrew services stop mongodbbrew services restart mongodb Mongo可视化工具RoboMongo/Robo 3T 、Studio 3T Redis 安装基于::内存::的高效的非关系型数据库 12345brew install redisbrew services start redisredis-server /usr/local/etc/redis.confbrew services stop redisbrew services restart redis 存储库的安装 PyMySQL 安装为了 Python 与 MySQL 进行交互 1pip3 install pymysql PyMongo 安装为了 Python 与 MongoDB 进行交互 1pip3 install pymongo redis-py 安装为了 Python 与 redis 进行交互 1pip3 install redis RedisDump 安装用于 Redis 数据导入/导出的工具，基于 Ruby 实现的。 123# 需要先安装 Rubybrew install rubygem install redis-dump Web 库的安装 Flask 安装轻量级的 Web 服务程序 1pip3 install flask Tornado 安装支持::异步::的 Web 框架，通过使用::非阻塞 I/O 流::，可以支撑成千上万的开发连接，效率非常高。 1pip3 install tornado App 爬取相关库的安装web 网页数据一般是通过请求服务器的接口来获取的，但对于 APP 主要使用一些::抓包技术::来抓取数据。 抓包工具：Charles &amp; mitmproxy：简单的接口mitmdump：复杂的接口，对抓取的请求和响应进行实时处理和保存Appium：像 selenium 一样对 APP 进行自动化控制。 Charles 安装网络抓包工具 mitmproxy 安装 Appium 安装 1234# 安装 node.js brew install node.js# 安装 appiumnpm install -g appium 爬虫框架的安装requests、selenium 库中的组件是可以复用的，抽离出来，将各个功能模块化，就慢慢形成了爬虫框架。 pyspider 安装 1234567pip3 install pyspider# Python3.7 把 async 当作关键之，所以需要修改 pyspider 源码中的 aynsc 字段名pyspider all # 启动# 访问 http://localhost:5000/# 如果安装异常，请先执行以下代码xcode-select --installsudo xcode-select -switch / Scrapy 安装 12pip3 install Scrapyscrapy Scrapy-Splash 安装Scrapy 中支持 ::JavaScript 渲染的工具::、使用 Splash 的 HTTP API 进行页面渲染 12345# 安装 dockerbrew cask install docker# 安装 splashdocker run -p 8050:8050 scrapinghub/splash # -d 参数：以守护态运行pip3 install scrapy-splash Scrapy-Redis 安装scrapy 的分布式扩展模块 1pip3 install scrapy-redis 部署相关库的安装大规模抓取数据的时候，一定会用到::分布式爬虫::：将一份代码，同时部署到多台主机上来协同运行。方式一：Scrapyd、Scrapyd-Client、Scrapyd API方式二：docker 集群部署 Docker 安装 一种容器技术，将::应用和环境::等进行::打包::，形成一个独立的”应用”。 12brew cask install dockersudo docker run hello-world 镜像加速默认是从国外的 Docker Hub 下载的，当然可以使用国内的镜像来加速下载 Scrapyd 安装一个用于部署和运行 Scrapy 项目的工具 1pip3 install scrapyd Scrapyd-Client 安装将 Scrapyd 代码部署到远程 Scrapyd 时，首先将代码打包为 EGG 文件，然后需要将 EGG 文件上传到远程主机。Scrapyd-Client 已经实现了这些功能。 12pip3 install scrapyd-clientscrapyd-deploy -h Scrapyd API 安装安装好 Scrapyd 之后，可以直接请求它提供的 ::API:: 来获取当前主机的 Scrapyd 任务运行状况。 1pip3 install python-scrapyd-api Scrapyrt 安装为 Scrapy 提供了一个::调度的 HTTP 接口::，可以直接请求 HTTP 接口来调度 Scrapy 任务。如果不需要分布式多任务的话，可以简单的使用 Scrapyrt 实现远程 Scrapy 任务的调度 1234567891011pip3 install scrapyrt# 在任意一个 Scrapy 项目中，执行以下命令来启动 HTTP 服务scrapyrt# 或者 Docker 启动：运行在 9080 端口，且本地 Scrapy 项目的路径为 ： /home/quotesbotdocker run -p 9080:9080 -tid -v /home/user/quotesbot:/scrapyrt/poject scrapinghub/scrapyrt``` * Gerapy 安装一个 Scrapy ::分布式管理模块::```shellpip3 install gerapy #学习/Python3网络爬虫开发实战/工具安装]]></content>
      <categories>
        <category>Python网络爬虫技术</category>
      </categories>
      <tags>
        <tag>工具</tag>
        <tag>Mac</tag>
        <tag>Python3</tag>
        <tag>网络爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MAC selenium webdriver 环境搭建]]></title>
    <url>%2F2019%2F01%2F12%2FPython%2FPython%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E6%8A%80%E6%9C%AF%2FMAC%20selenium%20webdriver%20%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[MAC selenium webdriver 环境搭建1、查看是否安装 python（本人系统：10.13.6，自带 Python 2.7.10）1python --version 2、安装并查看 pip12sudo easy_install pippip --version 3、安装 WebDriver（selenium）1sudo easy_install selenium # 貌似需要sudo安装 4、下载浏览器驱动（chrome/firefox/phantomjs） chrome 驱动下载网址：①、http://chromedriver.chromium.org/downloads ②、http://npm.taobao.org/mirrors/chromedriver/将文件保存至本地磁盘，比如：/Users/username/Tools/chromedriver移动到 usr/local/bin 目录下（确保 ~/.bash_profile 中已经加入 export PATH=/usr/local/bin:$PATH） 12cd /Users/username/Tools/mv chromedriver /usr/local/bin/ firefox 驱动下载网址：https://github.com/mozilla/geckodriver/releases brew 安装：如安装 phantomjs（注意：phantomjs 貌似不支持模拟移动设备）1brew update &amp;&amp; brew install phantomjs 5、开始写测试脚本123456789101112131415161718192021222324#!/usr/bin/python#coding:utf-8from selenium import webdriverimport time# 通过 executable_path 指定 chrome 驱动文件所在路径driver = webdriver.Chrome(executable_path="chromedriver")driver.set_window_size("400", "600")driver.implicitly_wait(10)url = "https://m.baidu.com"driver.get(url)elem = driver.find_elements_by_css_selector('#index-kw')# 在输入框中输入 javaelem[0].send_keys('java')# 等待是为了方便查看浏览器效果time.sleep(5)click = driver.find_elements_by_css_selector('#index-bn')# 点百度一下click[0].click()time.sleep(5)driver.quit() 注意：1、selenium 3.x 版本开始，不再提供默认浏览器支持，都是通过各个浏览器提供驱动进行支持。 2、注意各个驱动及浏览器的版本对应关系，否则将可能无法调起浏览器。 https://blog.csdn.net/xqhadoop/article/details/77892796 https://blog.csdn.net/huilan_same/article/details/51896672 异常处理：问题一： dyld: Library not loaded: /usr/local/opt/openssl/lib/libssl.1.0.0.dylib 解决：重装 openssl12brew remove opensslbrew install openssl]]></content>
      <categories>
        <category>Python网络爬虫技术</category>
      </categories>
      <tags>
        <tag>工具</tag>
        <tag>Mac</tag>
        <tag>selenium</tag>
        <tag>Python3</tag>
        <tag>网络爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Centos 7 部署 selenium + chrome + chromedriver]]></title>
    <url>%2F2018%2F12%2F24%2FPython%2FPython%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E6%8A%80%E6%9C%AF%2FCentos%207%20%E9%83%A8%E7%BD%B2%20selenium%20%2B%20chrome%20%2B%20chromedriver%2F</url>
    <content type="text"><![CDATA[Centos 7 部署 selenium + chrome + chromedriver版本要求：selenium &gt;= 3.14 确定可以访问外网12route # 查看默认网关route add default gw 网关（如：xx.xx.xx.1） NO.1 安装 chrome12curl https://intoli.com/install-google-chrome.sh | bashgoogle-chrome-stable --version # 查看版本，确认是否安装成功 NO.2 安装 chromedriver1234wget https://chromedriver.storage.googleapis.com/2.43/chromedriver_linux64.zipunzip chromedriver_linux64.zipsudo mv chromedriver /usr/local/bin/chromedriver -v # 查看版本，确认是否安装成功 NO.3 安装 java-812345678910wget --no-cookies --no-check-certificate --header "Cookie: gpw_e24=http%3A%2F%2Fwww.oracle.com%2F; oraclelicense=accept-securebackup-cookie" "http://download.oracle.com/otn-pub/java/jdk/8u141-b15/336fa29ff2bb4ef291e347e091f7f4a7/jdk-8u141-linux-x64.tar.gz"tar -xvf jdk-8u141-linux-x64.tar.gzsudo mv jdk1.8.0_141/ /opt/vim ~/.bashrc# 写入以下代码export JAVA_HOME=/opt/jdk1.8.0_141export PATH=$JAVA_HOME/bin:$PATH# 使配置生效source ~/.bashrc java -version NO.4 更新 yum 源 &amp; 安装 git12345678910111213141516cd /etc/yum.repos.dsudo wget http://mirrors.163.com/.help/CentOS6-Base-163.reposudo yum updatesudo vim /etc/yum.repos.d/wandisco-git.repo# 写入以下代码[wandisco-git]name=Wandisco GIT Repositorybaseurl=http://opensource.wandisco.com/centos/7/git/$basearch/enabled=1gpgcheck=1gpgkey=http://opensource.wandisco.com/RPM-GPG-KEY-WANdiscosudo rpm --import http://opensource.wandisco.com/RPM-GPG-KEY-WANdiscosudo yum install gitssh-keygen -t rsacat ~/.ssh/id_rsa.pub NO.5 安装 pip123sudo yum -y install epel-releasesudo yum install python-pipsudo pip install --upgrade pip NO.6 安装 selenium 并下载 selenium-server-standalone-3.14.0.jar1pip install --user selenium]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>工具</tag>
        <tag>chrome</tag>
        <tag>selenium</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Celery signature]]></title>
    <url>%2F2018%2F11%2F04%2FCelery%2FCelery%20signature%2F</url>
    <content type="text"><![CDATA[Celery signature什么是 signature？前面介绍了可以通过 delay 和 apply_async 来执行一个任务，多数情况下这已经足够使用，但是有时候你希望能够将任务调用的签名传递给另一个进程或者作为另一个函数的签名时，现有的方法就不够用了。 在 Celery 中，任务签名包含了一次任务调用的参数、关键字参数以及执行选项信息，它可以传递给其他函数，甚至序列化后通过网络传输。 如何创建 signature？通过任务的 signature 方法，创建任务签名对象12&gt;&gt;&gt; add.signature((2, 2), countdown=10)tasks.add(2, 2) 通过快捷方法，创建任务签名对象：12&gt;&gt;&gt; add.s(2, 2)tasks.add(2, 2) 如何执行 signature？直接执行签名在当前进程中执行 12&gt;&gt;&gt; s_add = add.signature((2, 2), countdown=10)&gt;&gt;&gt; s_add() Worker 执行签名在 Worker 任务进程中执行 12&gt;&gt;&gt; s_add = add.s(2, 2)&gt;&gt;&gt; s_add.delay() 跟一般的任务函数对比，区别在于签名可能已经指定了参数签名。 该add任务有两个参数，因此指定两个参数的签名将构成一个完整的签名：1234&gt;&gt;&gt; s1 = add.s(2, 2)&gt;&gt;&gt; res = s1.delay()&gt;&gt;&gt; res.get()4 但是，您也可以创建不完整的签名来创建的我们称之为 partials（偏函数）12# incomplete partial: add(?, 2)&gt;&gt;&gt; s2 = add.s(2) s2 现在是一个部分签名，需要另一个参数完成，这可以在调用签名时解决：1234# resolves the partial: add(8, 2)&gt;&gt;&gt; res = s2.delay(8)&gt;&gt;&gt; res.get()10 在这里，你添加参数8，形成了完整的签名。add(8, 2) 关键字参数也可以在以后添加，覆盖掉任何现有的关键字参数12&gt;&gt;&gt; s3 = add.s(2, 2, debug=True)&gt;&gt;&gt; s3.delay(debug=False) # debug is now False.]]></content>
      <categories>
        <category>Python</category>
        <category>Celery</category>
      </categories>
      <tags>
        <tag>Celery</tag>
        <tag>Python</tag>
        <tag>分布式任务队列</tag>
        <tag>签名</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Celery 定时任务]]></title>
    <url>%2F2018%2F11%2F04%2FCelery%2FCelery%20%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[Celery 定时任务Celery 除了可以执行异步任务，也支持执行周期性任务（Periodic Tasks），或者说定时任务。Celery Beat 进程通过读取配置文件的内容，周期性地将定时任务发往任务队列。 让我们看看例子，项目结构如下： 123456celery_demo # 项目根目录 ├── celery_app # 存放 celery 相关文件 ├── __init__.py ├── celeryconfig.py # 配置文件 ├── task1.py # 任务文件 └── task2.py # 任务文件 __init__.py 代码如下： 1234567#!/usr/bin/env python# -*- coding: utf-8 -*-from celery import Celeryapp = Celery('demo') # 创建 Celery 实例app.config_from_object('celery_app.celeryconfig') # 通过 Celery 实例加载配置模块 celeryconfig.py 代码如下： 123456789101112131415161718192021222324252627282930313233#!/usr/bin/env python# -*- coding: utf-8 -*-from datetime import timedeltafrom celery.schedules import crontab# Broker and BackendBROKER_URL = 'redis://127.0.0.1:6379'CELERY_RESULT_BACKEND = 'redis://127.0.0.1:6379/0'# TimezoneCELERY_TIMEZONE='Asia/Shanghai' # 指定时区，不指定默认为 'UTC'# CELERY_TIMEZONE='UTC'# importCELERY_IMPORTS = ( 'celery_app.task1', 'celery_app.task2')# schedulesCELERYBEAT_SCHEDULE = &#123; 'add-every-30-seconds': &#123; 'task': 'celery_app.task1.add', 'schedule': timedelta(seconds=30), # 每 30 秒执行一次 'args': (5, 8) # 任务函数参数 &#125;, 'multiply-at-some-time': &#123; 'task': 'celery_app.task2.multiply', 'schedule': crontab(hour=9, minute=50), # 每天早上 9 点 50 分执行一次 'args': (3, 7) # 任务函数参数 &#125;&#125; task1.py 代码如下：12345678910#!/usr/bin/env python# -*- coding: utf-8 -*-import timefrom celery_app import app@app.taskdef add(x, y): time.sleep(2) return x + y task2.py 代码如下：12345678910#!/usr/bin/env python# -*- coding: utf-8 -*-import timefrom celery_app import app@app.taskdef multiply(x, y): time.sleep(2) return x * y 现在，让我们启动 Celery Worker 进程，在项目的根目录下执行下面命令： 1TinyDolphin:celery_demo zhouyonglong01$ celery -A celery_app worker --loglevel=info 接着，启动 Celery Beat 进程，定时将任务发送到 Broker，在项目根目录下执行下面命令：1TinyDolphin:celery_demo zhouyonglong01$ celery beat -A celery_app 启动成功，看到如下日志：12345678910celery beat v4.2.1 (windowlicker) is starting.__ - ... __ - _LocalTime -&gt; 2018-11-04 16:55:42Configuration -&gt; . broker -&gt; amqp://guest:**@localhost:5672// . loader -&gt; celery.loaders.app.AppLoader . scheduler -&gt; celery.beat.PersistentScheduler . db -&gt; celerybeat-schedule . logfile -&gt; [stderr]@%WARNING . maxinterval -&gt; 5.00 minutes (300s) 之后，在 Worker 窗口我们可以看到，任务 task1 每 30 秒执行一次，而 task2 每天早上 9 点 50 分执行一次。123456[2018-11-04 16:56:13,233: INFO/MainProcess] Received task: celery_app.task1.add[3e9324d4-a27b-44a1-b530-d7dcb2c4f27a][2018-11-04 16:56:15,254: INFO/ForkPoolWorker-2] Task celery_app.task1.add[3e9324d4-a27b-44a1-b530-d7dcb2c4f27a] succeeded in 2.01500433599s: 13[2018-11-04 16:56:43,082: INFO/MainProcess] Received task: celery_app.task1.add[e7e1d632-23d9-4c26-9292-d6d56fef70c4][2018-11-04 16:56:45,098: INFO/ForkPoolWorker-3] Task celery_app.task1.add[e7e1d632-23d9-4c26-9292-d6d56fef70c4] succeeded in 2.01342217601s: 13[2018-11-04 16:57:13,082: INFO/MainProcess] Received task: celery_app.task1.add[88467637-a0a7-4357-a17a-b9e564d65d05][2018-11-04 16:57:15,085: INFO/ForkPoolWorker-2] Task celery_app.task1.add[88467637-a0a7-4357-a17a-b9e564d65d05] succeeded in 2.00195308999s: 13 PS：在上面，我们用两个命令启动了 Worker 进程和 Beat 进程，我们也可以将它们放在一个命令中：1TinyDolphin:celery_demo zhouyonglong01$ celery -B -A celery_app worker --loglevel=info Celery 周期性任务也有多个配置项，可参考官方文档。]]></content>
      <categories>
        <category>Python</category>
        <category>Celery</category>
      </categories>
      <tags>
        <tag>Celery</tag>
        <tag>Python</tag>
        <tag>分布式任务队列</tag>
        <tag>定时任务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Celery 调用方法 delay & apply_async 的区别]]></title>
    <url>%2F2018%2F11%2F04%2FCelery%2FCelery%20%E8%B0%83%E7%94%A8%E6%96%B9%E6%B3%95%20delay%20%26%20apply_async%20%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[Celery 调用方法 delay &amp; apply_async 的区别之前的例子，我们使用了 delay() 或 apply_async() 方法来调用任务。事实上，delay 方法封装了 apply_async，如下： 123def delay(self, *partial_args, **partial_kwargs): """Shortcut to :meth:`apply_async` using star arguments.""" return self.apply_async(partial_args, partial_kwargs) 也就是说，delay 是使用 apply_async 的快捷方式。apply_async 支持更多的参数，它的一般形式如下1apply_async(args=(), kwargs=&#123;&#125;, route_name=None, **options) apply_async 常用的参数如下： countdown：指定多少秒后执行任务 12truetask1.apply_async(args=(2, 3), countdown=5) # 5 秒后执行任务 eta (estimated time of arrival)：指定任务被调度的具体时间，参数类型是 datetime 123from datetime import datetime, timedelta# 当前 UTC 时间再加 10 秒后执行任务task1.multiply.apply_async(args=[3, 7], eta=datetime.utcnow() + timedelta(seconds=10)) expires：任务过期时间，参数类型可以是 int，也可以是 datetime 1task1.multiply.apply_async(args=[3, 7], expires=10) # 10 秒后过期 更多的参数列表可以在官方文档中查看。]]></content>
      <categories>
        <category>Python</category>
        <category>Celery</category>
      </categories>
      <tags>
        <tag>Celery</tag>
        <tag>Python</tag>
        <tag>分布式任务队列</tag>
        <tag>调用方法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Celery 使用配置]]></title>
    <url>%2F2018%2F11%2F04%2FCelery%2FCelery%20%E4%BD%BF%E7%94%A8%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[Celery 使用配置在较大的项目中，采用独立配置模块（中心化保存配置）更为有效，通常我们把文件命名为 celeryconfig.py。Celery 的配置比较多，可以在官方文档查询每个配置项的含义。 下面，我们再看一个例子。项目结构如下：1234567celery_demo # 项目根目录 ├── celery_app # 存放 celery 相关文件 │ ├── __init__.py │ ├── celeryconfig.py # 配置文件 │ ├── task1.py # 任务文件 1 │ └── task2.py # 任务文件 2 └── client.py # 应用程序 __init__.py 代码如下： 1234567#!/usr/bin/env python# -*- coding: utf-8 -*-from celery import Celeryapp = Celery('demo') # 创建 Celery 实例app.config_from_object('celery_app.celeryconfig') # 通过 Celery 实例加载配置模块 celeryconfig.py 代码如下： 1234567891011121314#!/usr/bin/env python# -*- coding: utf-8 -*-BROKER_URL = 'amqp://guest@localhost:5672//' # 指定 BrokerCELERY_RESULT_BACKEND = 'redis://127.0.0.1:6379' # 指定 BackendCELERY_TIMEZONE='Asia/Shanghai' # 指定时区，默认是 UTC# CELERY_TIMEZONE='UTC' CELERY_IMPORTS = ( # 指定导入的任务模块 'celery_app.task1', 'celery_app.task2') task1.py 代码如下：12345678910#!/usr/bin/env python# -*- coding: utf-8 -*-import timefrom celery_app import app@app.taskdef add(x, y): time.sleep(2) return x + y task2.py 代码如下：12345678910#!/usr/bin/env python# -*- coding: utf-8 -*-import timefrom celery_app import app@app.taskdef multiply(x, y): time.sleep(2) return x * y client.py 代码如下：123456789#!/usr/bin/env python# -*- coding: utf-8 -*-from celery_app import task1from celery_app import task2task1.add.apply_async(args=[2, 8]) # 也可用 task1.add.delay(2, 8)task2.multiply.apply_async(args=[3, 7]) # 也可用 task2.multiply.delay(3, 7)print 'hello world' 现在，让我们启动 Celery Worker 进程，在项目的根目录下执行下面命令：1TinyDolphin:celery_demo zhouyonglong01$ celery -A celery_app worker --loglevel=info 接着，运行 python client.py，它会发送两个异步任务到 Broker 12TinyDolphin:celery_demo zhouyonglong01$ python client.pyhello world 在 Worker 的窗口我们可以看到如下输出： 1234[2018-11-04 16:15:26,449: INFO/MainProcess] Received task: celery_app.task1.add[44051062-fdf9-41d0-bccb-6a393c1f2eab][2018-11-04 16:15:26,450: INFO/MainProcess] Received task: celery_app.task2.multiply[b3183cc3-0565-4359-b21f-d7fb7c2e22a2][2018-11-04 16:15:28,466: INFO/ForkPoolWorker-2] Task celery_app.task1.add[44051062-fdf9-41d0-bccb-6a393c1f2eab] succeeded in 2.012861372s: 10[2018-11-04 16:15:28,466: INFO/ForkPoolWorker-4] Task celery_app.task2.multiply[b3183cc3-0565-4359-b21f-d7fb7c2e22a2] succeeded in 2.01327455899s: 21]]></content>
      <categories>
        <category>Python</category>
        <category>Celery</category>
      </categories>
      <tags>
        <tag>Celery</tag>
        <tag>Python</tag>
        <tag>分布式任务队列</tag>
        <tag>配置文件</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Celery 异步任务]]></title>
    <url>%2F2018%2F11%2F04%2FCelery%2FCelery%20%E5%BC%82%E6%AD%A5%E4%BB%BB%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[Celery 异步任务使用 Celery 实现异步任务主要有三个步骤： 创建一个 Celery 实例； 启动 Celery worker； 应用程序调用异步任务。 准备工作选择 broker &amp; backend RabbitMQ功能齐全，稳定，耐用且易于安装。它是生产环境的绝佳选择。（官网推荐使用） Redis功能齐全，但在突然终止或电源故障时更容易丢失数据 如果选择以上两种 broker，则需要分别安装，可参考Mac 安装 Redis &amp; Mac 安装 Rabbitmq 。 安装 Celery1pip install celery 创建 Celery 任务将下面的代码保存为文件 tasks.py： 123456789101112131415161718192021#!/usr/bin/env python# -*- coding: utf-8 -*-from celery import Celeryimport timebroker = 'amqp://guest@localhost:5672//'backend = 'redis://127.0.0.1:6379'# 创建 Celery 实例# 第一个参数：当前模板的名称。只有在 __main__ 模板中定义任务时才能自动生成名称。# 第二个参数：指定使用的消息中间件的 URL# 第三个参数：指定任务结果存储的 URLapp = Celery('tasks', broker=broker, backend=backend)# 定义一个名为 add 的单个任务：返回两个数的总和# 创建了一个 Celery 任务 add，当函数被 @app.task 装饰后，就成为可被 Celery 调度的任务@app.taskdef add(x, y): time.sleep(5) # 模拟耗时操作 return x + y 启动 Celery worker在当前目录下，使用以下指令启动 Celery worker：1celery worker -A tasks --loglevel=info 其中， 参数 -A 指定了 Celery 实例的位置，本例是在 tasks.py 中，Celery 会自动在该文件中寻找 Celery 对象实例，当然，我们也可以自己指定，在本例，使用 -A tasks.app。 参数 –loglevel 指定了日志级别，默认为 warning，也可以使用 -l info 来表示； 启动成功后，控制台会显示如下输出： 调度任务现在，我们可以在应用程序中使用 delay() 或 apply_async() 方法来调用任务。 在当前目录打开 Python 控制台，输入以下代码： 123&gt;&gt;&gt; from tasks import add&gt;&gt;&gt; add.delay(1, 20)&lt;AsyncResult: d91cb44d-e4b5-489b-9dbf-982ae2b39e0d&gt; 在上面，我们从 tasks.py 文件中导入了 add 任务对象，然后使用 delay() 方法将任务发送到消息中间件（Broker），Celery Worker 进程监控到该任务后，就会进行执行。我们将窗口切换到 Worker 的启动窗口，会看到多了两条日志： 12[2018-11-04 15:43:25,351: INFO/MainProcess] Received task: tasks.add[d91cb44d-e4b5-489b-9dbf-982ae2b39e0d][2018-11-04 15:43:25,386: INFO/ForkPoolWorker-2] Task tasks.add[d91cb44d-e4b5-489b-9dbf-982ae2b39e0d] succeeded in 0.0192078250111s: 21 这说明任务已经被调度并执行成功。 PS：我们如果想获取执行后的结果，可以这样做： 123456789&gt;&gt;&gt; result = add.delay(10, 42)&gt;&gt;&gt; result.ready() # 使用 ready() 判断任务是否执行完毕False&gt;&gt;&gt; result.ready()False&gt;&gt;&gt; result.ready()True&gt;&gt;&gt; result.get() # 使用 get() 获取任务结果52 注意：虽然执行 add 方法 5s 之后才会返回结果，但是这是一个异步任务，不会阻塞主程序的，因此主程序并不会等待，而是会继续向下执行。 后台启动 Celery worker123456789101112celery multi start worker -A tasks -l infocelery multi restart worker -A tasks -l info# 不等待worker关闭celery multi stop worker -A tasks -l info# 等待worker关闭celery multi stopwait worker -A tasks -l infocelery multi start w1 -A project -l infocelery multi start w2 -A project -l infocelery multi start w3 -A project -l info# 立即停止w1,w2celery multi stop w1 w2 注意：celery multi 不会存储有关 worker 的信息，因此在重启时，需要使用相同的命令行参数。停止时，只能使用相同的 pidfile 和 logfile 参数。 默认情况下，它会在当前目录中创建pid和日志文件，以防止多个工作人员在彼此之上启动，但是推荐你将这些文件放在专用目录下。 123mkdir -p /var/run/celerymkdir -p /var/log/celerycelery multi start w1 -A proj -l info --pidfile=/var/run/celery/%n.pid --logfile=/var/log/celery/%n%I.log 使用multi命令可以启动多个worker，并且还有一个强大的命令行语法来为不同的worker指定参数1celery multi start 10 -A proj -l info -Q:1-3 images,video -Q:4,5 data -Q default -L:4,5 debug 有关更多示例，请参考官方文档。]]></content>
      <categories>
        <category>Python</category>
        <category>Celery</category>
      </categories>
      <tags>
        <tag>Celery</tag>
        <tag>Python</tag>
        <tag>分布式任务队列</tag>
        <tag>异步</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Celery 框架及其使用场景]]></title>
    <url>%2F2018%2F11%2F04%2FCelery%2FCelery%20%E6%A1%86%E6%9E%B6%E5%8F%8A%E5%85%B6%E4%BD%BF%E7%94%A8%E5%9C%BA%E6%99%AF%2F</url>
    <content type="text"><![CDATA[Celery 框架及其使用场景celery 是一款基于 python 开发的、专注于实时处理和任务调度的分布式任务队列。 Celery 架构以及组成部分 可以看到，Celery 主要包含以下几个模块： 任务模板 Task异步任务：通常在业务逻辑中被触发并发往任务队列。定时任务：由 Celery Beat 进程周期性地将任务发往任务队列。 消息中间件 Broker即为任务调度队列，接收任务生产者发来的消息（即任务），将任务存入队列，再按序分发给任务消费者。Celery 本身不提供队列服务，官方推荐使用 RabbitMQ 和 Redis 等。 任务执行单元 WorkerWorker 是执行任务的处理单元，它实时监控消息队列，获取队列中调度的任务，并执行它。 任务结果存储 BackendBackend 用于存储任务的执行结果，以供查询。同消息中间件一样，存储也可使用 RabbitMQ, Redis 和 MongoDB 等。 使用场景例子一：在程序的运行过程中，经常会碰到一些耗时耗资源的操作，为了避免它们阻塞主程序的运行，我们经常会采用多线程或异步任务。 比如：在 web 开发中，对新用户的注册，我们通常会给他发一封激活邮件，而发邮件就是 IO阻塞式任务，如果它们直接放到应用当中，就需要等到邮件发出去之后，才能进行下一步操作，此时用户只有一直等待。 celery 解决方案：在业务逻辑中触发一个发邮件的异步任务，而主程序可以继续往下运行。 例子二：在程序运行的过程中，有一个要运行很久的任务，但是我们又不想阻塞主程序。 解决方案：使用多线程。但是当并发量过大时，多线程也会扛不住 继续解决：使用线程池来限制并发数。 多线程对于共享资源的使用也是比较麻烦的一件事。 对于协程，它还是在同一个线程中执行的，如果一个任务本身的执行时间很长，而不是因为等待 IO 被挂起的，那么也同样会阻塞其他的协程。 最终方案：基于以上的问题，我们可以使用一个强大的分布式任务队列 Celery 来让任务的执行和主程序完全的脱离，甚至不在同一个主机内。通过队列来调度任务，所以不用担心并发量高时导致系统负载过大。 例子三：执行和主程序脱离 你想对100台机器执行一条批量命令，可能会花很长时间 ，但你不想让你的程序等着结果返回，而是给你返回一个任务ID,你过一段时间只需要拿着这个任务id就可以拿到任务执行结果，这样的话，在任务执行进行时，你可以继续做其它的事情。 例子四：做一个定时任务 比如每天定时检测一下你们所有客户的资料，如果发现今天是客户的生日，就给他发个短信祝福]]></content>
      <categories>
        <category>Python</category>
        <category>Celery</category>
      </categories>
      <tags>
        <tag>Celery</tag>
        <tag>Python</tag>
        <tag>分布式任务队列</tag>
        <tag>框架</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[复杂度分析（下）：浅析最好、最坏、平均、均摊时间复杂度]]></title>
    <url>%2F2018%2F10%2F28%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%2F%E5%A4%8D%E6%9D%82%E5%BA%A6%E5%88%86%E6%9E%90%EF%BC%88%E4%B8%8B%EF%BC%89%EF%BC%9A%E6%B5%85%E6%9E%90%E6%9C%80%E5%A5%BD%E3%80%81%E6%9C%80%E5%9D%8F%E3%80%81%E5%B9%B3%E5%9D%87%E3%80%81%E5%9D%87%E6%91%8A%E6%97%B6%E9%97%B4%E5%A4%8D%E6%9D%82%E5%BA%A6%2F</url>
    <content type="text"><![CDATA[复杂度分析（下）：浅析最好、最坏、平均、均摊时间复杂度主要四个复杂度分析方面的知识点：最好情况时间复杂度、最坏情况时间复杂度、平均情况时间复杂度、均摊时间复杂度 最好、最坏情况时间复杂度123456789101112// n 表示数组 array 的长度int find(int[] array, int n, int x) &#123; int i = 0; int pos = -1; for (; i &lt; n; ++i) &#123; if (array[i] == x) &#123; pos = i; break; &#125; &#125; return pos;&#125; 很显然，我们上一节简单的分析方法，解决不了这个问题。 第一种情况：如果数组中第一个元素正好是要查找的变量 x ，那么就不需要继续遍历剩下的 n - 1 个数据了，那么时间复杂度：O(1) 第二种情况：如果数组中不存在要查找的变量 x ，那就需要把整个数组都遍历一遍，时间复杂度就成了：O(n) 所以：不同的情况下，这段代码的复杂度不一样。 ①、最好情况时间复杂度就是，在最理想的情况下，执行这段代码的时间复杂度。 ②、最坏情况时间复杂度就是，在最糟糕的情况下，执行这段代码的时间复杂度。 平均情况时间复杂度不考虑变量 x 在数组中出现的概率问题（结论正确，但是分析过程有点问题）拿上面的代码来分析，要查找变量 x 在数组中的位置，有 n + 1 种情况：在数组 0 ~ n -1 位置中和不在数组中。这样的话，我们把每种情况下，查找需要遍历的元素个数累加起来，然后再除以 n + 1 ，就可以得到需要遍历的元素个数的平均值了，即：(1 + 2 + 3 …… + n + n ) / (n+1) = n(n+3) / 2(n+1) 。 但是在时间复杂度大 O 标记法中，可以省略掉系数、低阶、常量，所以简化之后，得到的平均时间复杂度就是 O(n)。 考虑变量 x 在数组中出现的概率问题为了方便理解，我们假设在数组中和不在数组中的概率都为 1/2 。要查找的数据出现在 0 ~ n-1 这 n 个位置的概率也是一样的，为 1/n 。所以，根据概率乘法法则，要查找的元素出现在 0 ~ n-1 中任意位置的概率为 1/(2n)。 那么考虑概率的话，计算过程：1 1/(2n) + 2 1/(2n) + 3 1/(2n) + …… + n 1/(2n) + n * 1/2 = (1+3n)/4 ，用大 O 表示法来表示，这段代码的加权平均时间复杂度仍然是 O(n) 。 这个值就是概率论中的加权平均值，也叫做期望值，所以平均时间复杂度的全称应该叫加权平均时间复杂度或者期望时间复杂度。 均摊时间复杂度123456789101112131415161718// 实现的功能：往数组中插入数据。当数组满了之后，我们用 for 循环遍历数组求和，并清空数组，将求和之后的 sum 值放到数组的第一个位置，再将新的数据插入。// array 表示一个长度为 n 的数组// 代码中的 array.length 就等于 nint[] array = new int[n];int count = 0;void insert(int val) &#123; if (count == array.length) &#123; int sum = 0; for (int i = 0; i &lt; array.length; ++i) &#123; sum = sum + array[i]; &#125; array[0] = sum; count = 1; &#125; array[count] = val; ++count;&#125; 利用之前的方式去分析时间复杂度最理想的情况下，数组中有空闲空间，我们只需要将数据插入到数组下标为 count 的位置即可，所以最好情况时间复杂度为 O(1)。 最坏的情况下，数组中没有空闲空间，我们需要做一次数组遍历求和，然后再将数据插入，所以最坏的情况为 O(n) 平均时间复杂度下，根据数据插入的位置不同，我们可以分为 n 种情况，每种情况的时间复杂度为 O(1) 。除此之外，还有一种”额外”的情况，就是在数组没有空闲空间的插入一个数据时，时间复杂度为 O(n)，而且这 n + 1 种情况发生的概率一样，都是 1/(n+1) 。所以，根据加权平均的计算方法，我们可以得到平均时间复杂度：1 1/(n+1) + 1 1/(n+1) + 1 1/(n+1) …… 1 1/(n+1) + n * 1/(n+1) = 2n/(n+1) —&gt; O(1) 利用摊还分析法去分析时间复杂度①、首先来看看 insert() 与 find() 的区别： find() 函数在极端的情况下，复杂度才为 O(1) ，但 insert() 在大多数情况下，时间复杂度都为 O(1)，只有个别情况下，复杂度才为 O (n) ； 对于 insert() 函数来说，O(1) 时间复杂度的插入和O(n) 时间复杂度的插入，出现的频率非常的有规律，而且有一定的前后时序关系，一般都是一个 O(n) 插入之后，紧跟 n-1 个O(1) 的插入操作，循环往复。 基于以上两种不同，我们可以引入一种更加简单的分析方法：摊还分析法。通过这种方法得到的时间复杂度，就叫做：均摊时间复杂度。 ②、如何使用摊还分析法来分析算法的均摊时间复杂度 大致思路：每一次 O(n) 的插入操作，都会跟着 n-1 次 O(1) 的插入操作，所以把耗时多的那次操作均摊到接下来的 n-1 次耗时小的操作上，均摊下来，这一组的连续的操作的均摊时间复杂度就是 O(1) 。 均摊时间复杂度和摊还分析应用场景比较特殊，所以我们并不会经常用到。其实均摊时间复杂度就是一种特殊的平均时间复杂度 应用场景对一个数据结构进行一组连续的操作中，大部分情况下时间复杂度都很低，只有个别情况下时间复杂度比较高，而且这些操作存在前后连贯的时序关系，这个时候，我们就可以将这一组操作放在一块分析，看是否能够将较高时间复杂度那次操作的耗时，平摊到其他那些时间复杂度比较低的操作上。而且这种情况下，一般均摊时间复杂度就等于最好情况时间复杂度。]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[复杂度分析（上）：如何分析、统计算法的执行效率和资源消耗？]]></title>
    <url>%2F2018%2F10%2F28%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%2F%E5%A4%8D%E6%9D%82%E5%BA%A6%E5%88%86%E6%9E%90%EF%BC%88%E4%B8%8A%EF%BC%89%EF%BC%9A%E5%A6%82%E4%BD%95%E5%88%86%E6%9E%90%E3%80%81%E7%BB%9F%E8%AE%A1%E7%AE%97%E6%B3%95%E7%9A%84%E6%89%A7%E8%A1%8C%E6%95%88%E7%8E%87%E5%92%8C%E8%B5%84%E6%BA%90%E6%B6%88%E8%80%97%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[复杂度分析（上）：如何分析、统计算法的执行效率和资源消耗？复杂度分析是整个算法学习的精髓，只要掌握了它，数据结构和算法的内容基本上就掌握了一半 Q：为什么需要复杂度分析？ A：首页对于这个问题有所疑惑：明明可以把代码跑一遍，通过统计、监控，就能得到算法执行的时间和占用的内存大小（某些书中的”事后统计法”）。为什么还需要复杂度分析呢？ 因为这种方法存在很大的局限性： ①、测试结果非常的依赖测试环境 不同的测试环境，得到的结果可能完全不同。 ②、测试结果受数据规模的影响很大 比如：排序算法中，对于不一样的待排序数据有序度，排序的执行时间就有很大的差别。（对于小规模的数据排序，插入排序可能反倒会比快速排序要快！） 所以，我们需要一个不用具体的测试数据来测试，就可以粗略地估算出算法的执行效率的方法——时间、空间复杂度分析。 大 O 复杂度表示法我们假设每一段代码的执行时间为 t 12345678int cal(int n) &#123; int sum = 0; // 一个 t 的执行时间 int i = 1; // t for (; i &lt;= n; ++i) &#123; // n * t sum = sum + i; // n * t &#125; return sum;&#125; 所以这段代码的总的执行时间 T(n) = (2n+2)*t 。可以看出：总的代码执行时间 T(n) 与每行代码的执行次数成正比。 所以，这段代码的大 O 时间复杂度表示法： T(n) = O(f(n)) 其中：T(n)：代码总的执行时间、n：数据规模的大小、f(n) 每行代码执行的次数总和、O ：代码的执行时间 T(n) 与 f(n) 表达式成正比 大 O 时间复杂度表示法：并不具体表示代码的真正执行时间，而是表示代码执行时间随着数据规模增长的变化趋势。也叫 渐进时间复杂度，简称时间复杂度 时间复杂度分析1、只关注循环执行次数最大的一段代码既然说大 O 复杂度是一种变化趋势，那么我们通常会忽略掉公式中的常量、低阶、系数，只需要记录一个最大阶的量级就可以了。 12345678int cal(int n) &#123; int sum = 0; // 常量级别的执行时间，与 n 无关，可以忽略 int i = 1; // 同上 for (; i &lt;= n; ++i) &#123; // n sum = sum + i; // n &#125; return sum;&#125; 所以以上代码，总的时间复杂度：O(n) 2、加法规则：总复杂度等于量级最大的那段代码的复杂度1234567891011121314151617181920212223242526int cal(int n) &#123;true// 常量级别 int sum_1 = 0; int p = 1; for (; p &lt; 100; ++p) &#123; sum_1 = sum_1 + p; &#125;true// O(n) 级别 int sum_2 = 0; int q = 1; for (; q &lt; n; ++q) &#123; sum_2 = sum_2 + q; &#125; // O(n²) 级别 int sum_3 = 0; int i = 1; int j = 1; for (; i &lt;= n; ++i) &#123; j = 1; for (; j &lt;= n; ++j) &#123; sum_3 = sum_3 + i * j; &#125; &#125; return sum_1 + sum_2 + sum_3; &#125; 所以以上代码，总的时间复杂度：T(n) = T1(n) + T2(n) + T3(n) = O(1) + O(n) + O(n²) = O(n²) 如果 T1(n) = O(f(n))、T2(n) = O(g(n))；那么T(n) = T1(n) + T2(n) = Max(O(f(n), g(n))) 3、乘法规则：嵌套代码的复杂度等于嵌套内外代码复杂度的乘积123456789101112131415161718int cal(int n) &#123;true// O(n) 级别 int ret = 0; int i = 1; for (; i &lt; n; ++i) &#123; ret = ret + f(i); &#125; &#125; // O(n) 级别 int f(int n) &#123; int sum = 0; int i = 1; for (; i &lt; n; ++i) &#123; sum = sum + i; &#125; return sum; &#125; 所以以上代码，总的时间复杂度：T = T1(n) T2(n) = O(n n) = O(n²) 如果 T1(n) = O(f(n))、T2(n) = O(g(n))；那么T(n) = T1(n) T2(n) = O(f(n) g(n)) 几种常见的时间复杂度实例分析1、多项式时间复杂度常量阶 O(1)、对数阶 O(logn)、线性阶 O(n)、线性对数阶 O(nlogn)、平方阶 O(n²)、立方阶 O(n³) …… K方阶 O(n^k) ①、O(1)一般情况下，只要算法中不存在循环语句，递归语句，就算有成千上万行代码，其时间复杂度依然是 O(1) ②、O(logn、nlogn)1234i=1;while (i &lt;= n) &#123; i = i * 2;&#125; 假设第三行代码的执行次数为 x ，则可以得到 2^x = n，进而得到 x = log2(n) 。所以这段代码的时间复杂度为 O(log2(n))。 由于前面我们说过，在使用大O标记复杂度的时候，可以忽略系数。所以，在对数阶时间复杂度的时候，我们忽略对数的底，统一表示为 O(logn) ③、O(n+m)、O(n*m)123456789101112131415int cal(int m, int n) &#123; int sum_1 = 0; int i = 1; for (; i &lt; m; ++i) &#123; sum_1 = sum_1 + i; &#125; int sum_2 = 0; int j = 1; for (; j &lt; n; ++j) &#123; sum_2 = sum_2 + j; &#125; return sum_1 + sum_2;&#125; 以上代码，出现两个无法预知 n、m 的数据规模量级大小，所以不能简单的利用加法法则，省略掉其中一个。所以以上代码的时间复杂度：T1(n) + T1(m) = O(f(n) + g(m)) = O(n+m)。 虽然此时加法法则无效，但是乘法法则依然有效：T1(n) T2(m) = O(f(n) g(m)) = O(n * m) 2、非多项式时间复杂度（非常低效的方法，直接略）指数阶 O(2^n)、阶乘阶 O(n!) 。 对于这个时间复杂度，会随着数据规模的增长，算法的执行时间会急剧增加。所以非常的低效。 空间复杂度分析前面讲过，时间复杂度的全称是渐进时间复杂度，表示算法的执行时间与数据规模之间的增长关系。那么类比一下，空间复杂度就是渐进空间复杂度，表示算法的存储空间与数据规模之间的增长关系。12345678910void print(int n) &#123; int i = 0; // 常量阶，申请了一个空间存储变量 i int[] a = new int[n]; // 一个大小为 n 的 int 型数组 for (i; i &lt;n; ++i) &#123; a[i] = i * i; &#125; for (i = n-1; i &gt;= 0; --i) &#123; print out a[i] &#125;&#125; 所以，以上的空间复杂度：O(n) 常见的空间复杂度：O(1)、O(n)、O(n²)，像 O(logn)、O(nlogn)这样的对数阶复杂度平时都用不到。 思考有人说，我们项目之前都会进行性能测试，再做代码的时间复杂度、空间复杂度分析，是不是多此一举呢？而且，每段代码都要分析一下时间复杂度、空间复杂度是不是很浪费时间呢？你怎么看待这个问题呢？]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mac 安装 brew 以及部分使用指令]]></title>
    <url>%2F2018%2F09%2F17%2F%E5%B7%A5%E5%85%B7%2FMac%20%E5%AE%89%E8%A3%85%20%20brew%20%E4%BB%A5%E5%8F%8A%E9%83%A8%E5%88%86%E4%BD%BF%E7%94%A8%E6%8C%87%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[Mac 安装 brew 以及部分使用指令NO.1 官网获取下载指令http://brew.sh/ Homebrew安装成功后，会自动创建目录 /usr/local/Cellar 来存放Homebrew安装的程序 NO.2 使用 brew 安装软件：brew install 软件名，例：brew install wget 搜索软件：brew search 软件名，例：brew search wget 卸载软件：brew uninstall 软件名，例：brew uninstall wget 更新所有软件：brew update 更新具体软件：brew upgrade 软件名 ，例：brew upgrade git 显示已安装软件：brew list 查看软件信息：brew info／home 软件名 ，例：brew info git ／ brew home gitPS：brew home指令是用浏览器打开官方网页查看软件信息 查看哪些已安装的程序需要更新： brew outdated 显示包依赖：brew reps 显示帮助：brew help]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>工具</tag>
        <tag>Mac</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mac 安装 Rabbitmq]]></title>
    <url>%2F2018%2F09%2F17%2F%E5%B7%A5%E5%85%B7%2FMac%20%E5%AE%89%E8%A3%85%20Rabbitmq%2F</url>
    <content type="text"><![CDATA[Mac 安装 RabbitmqNO.1 Mac 下安装 Rabbitmq1234/usr/bin/ruby -e "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)" # 安装 brew（Homebrew：Mac OS平台下的软件包管理工具）brew install rabbitmqsudo vim ~/.bash_profilePATH=$PATH:/usr/local/sbin # 最后一行加上 NO.2 启动 &amp; 停止 Rabbitmq12345678# 启动服务brew services start rabbitmq# 停止服务brew services stop rabbitmq# 启用插件rabbitmq-plugins enable rabbitmq_management# 禁用插件rabbitmq-plugins disable rabbitmq_management NO.3 进入控制台http://localhost:15672/ 默认用户名和密码：guest , guest]]></content>
      <categories>
        <category>工具</category>
        <category>Rabbitmq</category>
        <category>Celery</category>
      </categories>
      <tags>
        <tag>Celery</tag>
        <tag>工具</tag>
        <tag>Mac</tag>
        <tag>Rabbitmq</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mac 安装 Redis]]></title>
    <url>%2F2018%2F09%2F17%2F%E5%B7%A5%E5%85%B7%2FMac%20%E5%AE%89%E8%A3%85%20Redis%2F</url>
    <content type="text"><![CDATA[Mac 安装 RedisNO.1 Mac 下安装 Redis1brew install redis NO.2 启动 &amp; 停止 Rabbitmq12345678# 启动Redis服务brew services start redis# 关闭Redis服务brew services stop redis# 重启Redis服务brew services restart redis# 打开图形化界面redis-cli NO.3 redise的配置文件所在路径/usr/local/etc/redis.conf]]></content>
      <categories>
        <category>工具</category>
        <category>Redis</category>
        <category>Celery</category>
      </categories>
      <tags>
        <tag>Celery</tag>
        <tag>工具</tag>
        <tag>Mac</tag>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[POJ 1852 Ants]]></title>
    <url>%2F2018%2F09%2F15%2FACM%2FPOJ%2F1852%2F</url>
    <content type="text"><![CDATA[POJ 1852 Ants123456789101112131415161718192021222324252627282930313233343536373839404142/* POJ:1852 题意：简单的理解就是， N 只蚂蚁以 1cm/s 的速度在，一条长 L 厘米的竿子上爬行（不知道初始的爬行方向），当蚂蚁爬到竿子的端点时会掉落。 但竿子太细了，如果有两只蚂蚁相遇时，它们只能各自反向爬回去。 对于蚂蚁，我们只知道它们距离竿子左端的距离 xi，但不知道当前蚂蚁的朝向 请计算所有蚂蚁落下竿子所需要的最短时间和最长时间 迷惑条件：如果有两只蚂蚁相遇时，它们只能各自反向爬回去。 转换思路：其实不管相遇后各自反向爬 OR 相遇后保持原方向爬行，结果都是一样的。 这样的话，最短时间 = MAX（所有蚂蚁距离竿子端点的最近距离） 最长时间 = MAX（所有蚂蚁距离竿子端点的最远距离）*/#include "iostream"using namespace std;#define MAX_N 1000010 int t,n,l,x[MAX_N];int maxT, minT;void solve()&#123; minT = 0; maxT = 0; for(int i = 0; i &lt; n ; i++)&#123; minT = max(minT, min(x[i], l - x[i])); maxT = max(maxT, max(x[i], l - x[i])); &#125;&#125;int main()&#123; scanf("%d", &amp;t); while(t--)&#123; scanf("%d %d", &amp;l , &amp;n); for(int i = 0; i &lt; n; i++)&#123; scanf("%d", &amp;x[i]); &#125; solve(); printf("%d %d\n", minT, maxT); &#125; return 0;&#125;]]></content>
      <categories>
        <category>ACM</category>
      </categories>
      <tags>
        <tag>ACM</tag>
        <tag>POJ</tag>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JetBrains系开发工具激活]]></title>
    <url>%2F2018%2F09%2F11%2F%E5%B7%A5%E5%85%B7%2FJetBrains%E7%B3%BB%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7%E6%BF%80%E6%B4%BB%2F</url>
    <content type="text"><![CDATA[NO.1 将“0.0.0.0 account.jetbrains.com”添加到 hosts 文件末尾处 hosts 文件所在目录： Windows：C:\Windows\System32\drivers\etc\hosts Mac：/etc/hosts NO.2 进入 http://idea.lanyus.com/ ，获得注册码即可。]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>工具</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mac终端添加ll、la、l命令]]></title>
    <url>%2F2018%2F09%2F10%2F%E5%B7%A5%E5%85%B7%2FMac%E7%BB%88%E7%AB%AF%E6%B7%BB%E5%8A%A0ll%E3%80%81la%E3%80%81l%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[Mac终端添加ll、la、l命令NO.1 编辑.bash_profile文件1vim ~/.bash_profile NO.2 添加别名映射关系123alias ll='ls -alF'alias la='ls -A'alias l='ls -CF' NO.3 source 文件1source ~/.bash_profile]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>工具</tag>
        <tag>Mac</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mac如何重置root用户密码]]></title>
    <url>%2F2018%2F09%2F10%2F%E5%B7%A5%E5%85%B7%2FMac%E5%A6%82%E4%BD%95%E9%87%8D%E7%BD%AEroot%E7%94%A8%E6%88%B7%E5%AF%86%E7%A0%81%2F</url>
    <content type="text"><![CDATA[Mac如何重置root用户密码？1、打开终端，输入：sudo bash，提示输入当前用户密码 2、成功进入bash命令模式之后，输入 sudo passwd root 3、输入新的 root 密码即可]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>工具</tag>
        <tag>Mac</tag>
      </tags>
  </entry>
</search>
